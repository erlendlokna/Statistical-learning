---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Compulsory exercise 2: Group 18"
author: "Thomas RÃ¸dland, Erlend Lokna"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---

```{r setup, eval=TRUE,echo=FALSE, message=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")
```


```{r, eval=TRUE,echo=FALSE, message=FALSE}
library("ggplot2")
library("tidyverse")
library("GGally")
library("MASS")
library("caret")
library("glmnet")
library("pls")
library("gam")
library("e1071")
library("tree")
library("randomForest")
library("ggfortify")
library("leaps")
```

```{r, eval=TRUE}
set.seed(1)
boston <- scale(Boston, center=T, scale=T)

# split into training and rest sets
train.ind = sample(1:nrow(boston), 0.8 * nrow(boston))
boston.train = data.frame(boston[train.ind, ])
boston.test = data.frame(boston[-train.ind, ])
```

# Problem 1

## a)
```{r, eval=TRUE}
#forward subset selection:
regfit.fwd = regsubsets(medv~., data = boston.train, nvmax=4, method = "forward")
regfit.bcwd = regsubsets(medv~., data = boston.train, nvmax=4, method = "backward")
```
plotting the R-squared vs number of predictors for forward and backwards:
```{r}
#plotting the R-squared vs number of predictors
plot(summary(regfit.fwd)$adjr2, xlab='No. of Variables',ylab='R^2',
     main="forward",type='b', xlim=c(1,4))

```

```{r}
plot(summary(regfit.bcwd)$adjr2, xlab='No. of Variables',ylab='R^2',
     main="bacward",type='b', xlim=c(1,4))

```


## b)
```{r, eval=TRUE}
#the best four predictors:
coef(regfit.fwd, 4)
```
## c)
```{r, eval=TRUE}
#K-fold cross-validation (k=5) 
#setup:
df.X.std <- scale(dplyr::select(Boston, -medv))       
X.train <- as.matrix(df.X.std)[train.ind,] #converting to matrix. 
X.test <- as.matrix(df.X.std)[-train.ind,]
Y.train <- Boston[train.ind, "medv"]
Y.train = as.matrix(Y.train)
Y.test <- Boston[-train.ind, "medv"]
Y.test = as.matrix(Y.test)

#calculation using cv.glmnet():
kf5_cv = cv.glmnet(x=X.train, y=Y.train, alpha = 1, nfolds = 5)
```

```{r, eval=TRUE}
#plot:
plot(kf5_cv)
```
Lambda corresponding to minimal Mean-Squared error:
```{r, eval=TRUE}
kf5_cv$lambda.min
```
coefficients:
```{r, eval=TRUE}
coef(kf5_cv, s=kf5_cv$lambda.min)
```
## d)
1. True
2. False
3.
4. True

# Problem 2

```{r, eval=TRUE}
set.seed(1)

# load a synthetic dataset
id <- "1CWZYfrLOrFdrIZ6Hv73e3xxt0SFgU4Ph"  # google file ID
synthetic <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))

# split into training and test sets
train.ind = sample(1:nrow(synthetic), 0.8 * nrow(synthetic))
synthetic.train = data.frame(synthetic[train.ind, ])
synthetic.test = data.frame(synthetic[-train.ind, ])

# show head(..)  Y: response variable; X: predictor variable
head(synthetic)
```

### a)

Fitting PCR and PSLR on the synthetic.train data set:

```{r, eval=TRUE}
pcr_model <- pcr(Y~., data = synthetic.train, scale = TRUE, validation = "CV")
plsr_model <- plsr(Y~., data=synthetic.train, scale = TRUE, validation = "CV")
```

```{r, eval=TRUE}
validationplot(pcr_model, val.type="MSEP", main="PCR")
```

```{r, eval=TRUE}
validationplot(plsr_model, val.type="MSEP", main="PlSR")
```

### b)

We can clearly see that the PSLR method shrinks the MSEP quicker for fewer components when comparing to the PCR method. PCR is a unsupervised method while PSLR is supervised. 


# Problem 3

## a)
1.
2.
3. False (the extra term makes it less wiggly not more smooth?)
4. True (increase in k -> higher bias, lower variance ?)

## b)
using gam function to create a adaptive model:
```{r, eval=TRUE}
adpt_mod <- gam(medv ~rm  + s(ptratio, df=3) + s(lstat, df=2), data=boston.train)
```

plotting results:
```{r, eval=TRUE}
par(mfrow=c(1,3)) #to partition the Plotting Window
plot(adpt_mod, se=TRUE)
```

# Problem 4

## a)
1. False (?)
2. True
3. True
4. False

## b)

See figure below:

```{r, echo=FALSE, out.width="50%", fig.cap="Sketch"}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
knitr::include_graphics("./recursive_three.drawio.png")
```



## c)
```{r}
library(tidyverse)
library(palmerpenguins)  # Contains the data set 'penguins'.
data(penguins)

names(penguins) <- c("species", "island", "billL", "billD", "flipperL", "mass", "sex", 
    "year")

Penguins_reduced <- penguins %>% dplyr::mutate(mass = as.numeric(mass), flipperL = as.numeric(flipperL), 
    year = as.numeric(year)) %>% drop_na()

# We do not want 'year' in the data (this will not help for future predictions)
Penguins_reduced <- Penguins_reduced[, -c(8)]

set.seed(4268)
# 70% of the sample size for training set
training_set_size <- floor(0.7 * nrow(Penguins_reduced))
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)
train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]
```

#### i)

```{r}
penguins.tree <- tree(species ~ ., data=train, split = "gini")
plot(penguins.tree, type="uniform")
text(penguins.tree, pretty = 0)
```

#### ii)

```{r}
set.seed(123)
#10 fold 
cv.penguins <- cv.tree(penguins.tree, K = 10)
plot(cv.penguins$dev ~ cv.penguins$size, type = "b", lwd = 2, col = "red", 
    xlab = "Tree Size", ylab = "Deviance")
```

We see that the optimal tree contains 4 leaves. Creating the optimal tree with 4 leaves:

#### iii)

```{r}
prune.penguins <- prune.tree(penguins.tree, best = 4)
plot(prune.penguins, style="uniform", type = "proportional")
text(prune.penguins, pretty=0)
```

## d)

classification tree based on a more advanced method?


# Problem 5

## a)
1. True?
2. 
3.
4.


# Problem 6

```{r}
# load a synthetic dataset
id <- "1NJ1SuUBebl5P8rMSIwm_n3S8a7K43yP4" # google file ID
happiness <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),fileEncoding="UTF-8-BOM")

cols = c('Country.name', 
         'Ladder.score',  # happiness score
         'Logged.GDP.per.capita',  
         'Social.support', 
         'Healthy.life.expectancy', 
         'Freedom.to.make.life.choices',
         'Generosity',  # how generous people are
         'Perceptions.of.corruption')

# We continue with a subset of 8 columns:
happiness = subset(happiness, select = cols)
rownames(happiness) <- happiness[, c(1)]

# And we creat an X and a Y matrix
happiness.X = happiness[, -c(1, 2)]
happiness.Y = happiness[, c(1, 2)]
happiness.XY = happiness[, -c(1)]

# scale
happiness.X = data.frame(scale(happiness.X))

library(ggfortify)
pca_mat = prcomp(happiness.X, center = T, scale = T)
```

```{r}
# Score and loadings plot:
autoplot(pca_mat, data = happiness.X, colour = "Black", loadings = TRUE, loadings.colour = "red", 
    loadings.label = TRUE, loadings.label.size = 5, label = T, label.size = 1.5)
```

## a)
i)
First of all we observe that the "logged GDP", "social support", "healthy life expectancy" and "ladder score" practically have the same direction in the plot above. This implies a correlation, and we can assume that each one effects the other. We also observe that "freedom to make life choices" and "perception of corruption" is close to opposite in direction. This may be interpreted as countries having more freedom experience less corruption. Also there is a relation between corruption and generosity as the angle between the two directions are sufficient. The same can be said about the generosity and "logged GDP", "social support", "healthy life expectancy" and "ladder score".

ii)


## b)
i)
```{r}
library(ggpubr)

plotData <- data.frame(absPC1 = abs(data.frame(pca_mat$rotation)$PC1),
                       variables = c(colnames(happiness.X)))

ggbarplot(plotData, x = "variables", y = "absPC1",
          main="sorted order [PCA]",
          fill="#f1595f",
          sort.val = "desc",
          sort.by.groups = FALSE,
          x.text.angle = 90,
          orientation="horizontal"
          )
```

ii)
```{r}
plsr_mat <- plsr(Ladder.score~., data = happiness.XY, scale=T)
```

iii)
```{r}
plotData$absPC1_PLSR <- abs(plsr_mat$loadings[,c('Comp 1')])

ggbarplot(plotData, x = "variables", y = "absPC1_PLSR",
          main="sorted order [PLSR]",
          fill="#f1595f",
          sort.val = "desc",
          sort.by.groups = FALSE,
          x.text.angle = 90,
          orientation="horizontal"
          )
```
```{r}

plotData$difference <- abs(plotData$absPC1 - plotData$absPC1_PLSR)

ggbarplot(plotData, x = "variables", y = "difference",
          main="difference",
          fill="#f1595f",
          sort.val = "desc",
          sort.by.groups = FALSE,
          x.text.angle = 90,
          orientation="horizontal"
          )

```


iv)

The three most important predictors are:

1. Logged GDP per capita
2. Healthy life expectancy
3. Social support

## c)
1. False
2. False (https://datascience.stackexchange.com/questions/63697/will-k-means-clustering-converge-to-the-same-results-given-the-same-data-set)
3. 
4. True

## d)
i)
```{r}
K = 3  # your choice
km.out = kmeans(happiness.X, K)

autoplot(pca_mat, data = happiness.X, colour = km.out$cluster, label = T, label.size = 2, 
    loadings = F, loadings.colour = "blue",  loadings.label = F, loadings.label.size = 3)
```

Using k = 3, we satisfy the condition. Usa is in a different cluster than the Scandinavian countries.

ii)
To answer this question we can sort the ladder scores and visualise the cluster results:

```{r}
happiness.XY$countries <- rownames(happiness.XY) #addging country names in seperate coloumn. Makes the plot easier.

ggbarplot(happiness.XY, x = "countries", y = "Ladder.score",
          main="Sorted ladder score",
          fill= km.out$cluster,
          color= km.out$cluster,
          sort.val = "desc",
          sort.by.groups = FALSE,
          x.text.angle = 90,
          x.text.size = 1,
          ylab=FALSE
          ) + theme(text = element_text(size = 2)) + theme(title = element_text(size = 10))
```

The K-means algorithm clusters the countries that share similarities within all predictors for the happiness ladder score. As you see in the plot above the countries at the top of the ladder is clustered together. With K=3 the algorithm finds three classes of countries